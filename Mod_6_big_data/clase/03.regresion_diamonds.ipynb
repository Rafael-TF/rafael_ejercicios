{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib \n",
    "\n",
    "Machine Learning en pyspark. Similar a Scikit Learn pero en pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|\n",
      "| 0.29|Premium|    I|    VS2| 62.4| 58.0|  334| 4.2|4.23|2.63|\n",
      "| 0.31|   Good|    J|    SI2| 63.3| 58.0|  335|4.34|4.35|2.75|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('regresion_diamonds').getOrCreate()\n",
    "df = spark.createDataFrame(sns.load_dataset('diamonds'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module pyspark.ml.regression:\n",
      "\n",
      "class LinearRegression(_JavaRegressor, _LinearRegressionParams, pyspark.ml.util.JavaMLWritable, pyspark.ml.util.JavaMLReadable)\n",
      " |  LinearRegression(*, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |  \n",
      " |  Linear regression.\n",
      " |  \n",
      " |  The learning objective is to minimize the specified loss function, with regularization.\n",
      " |  This supports two kinds of loss:\n",
      " |  \n",
      " |  * squaredError (a.k.a squared loss)\n",
      " |  * huber (a hybrid of squared error for relatively small errors and absolute error for     relatively large ones, and we estimate the scale parameter from training data)\n",
      " |  \n",
      " |  This supports multiple types of regularization:\n",
      " |  \n",
      " |  * none (a.k.a. ordinary least squares)\n",
      " |  * L2 (ridge regression)\n",
      " |  * L1 (Lasso)\n",
      " |  * L2 + L1 (elastic net)\n",
      " |  \n",
      " |  .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Fitting with huber loss only supports none and L2 regularization.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from pyspark.ml.linalg import Vectors\n",
      " |  >>> df = spark.createDataFrame([\n",
      " |  ...     (1.0, 2.0, Vectors.dense(1.0)),\n",
      " |  ...     (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
      " |  >>> lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
      " |  >>> lr.setMaxIter(5)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getMaxIter()\n",
      " |  5\n",
      " |  >>> lr.setRegParam(0.1)\n",
      " |  LinearRegression...\n",
      " |  >>> lr.getRegParam()\n",
      " |  0.1\n",
      " |  >>> lr.setRegParam(0.0)\n",
      " |  LinearRegression...\n",
      " |  >>> model = lr.fit(df)\n",
      " |  >>> model.setFeaturesCol(\"features\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.setPredictionCol(\"newPrediction\")\n",
      " |  LinearRegressionModel...\n",
      " |  >>> model.getMaxIter()\n",
      " |  5\n",
      " |  >>> model.getMaxBlockSizeInMB()\n",
      " |  0.0\n",
      " |  >>> test0 = spark.createDataFrame([(Vectors.dense(-1.0),)], [\"features\"])\n",
      " |  >>> abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.coefficients[0] - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> abs(model.intercept - 0.0) < 0.001\n",
      " |  True\n",
      " |  >>> test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
      " |  >>> abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
      " |  True\n",
      " |  >>> lr.setParams(featuresCol=\"vector\")\n",
      " |  LinearRegression...\n",
      " |  >>> lr_path = temp_path + \"/lr\"\n",
      " |  >>> lr.save(lr_path)\n",
      " |  >>> lr2 = LinearRegression.load(lr_path)\n",
      " |  >>> lr2.getMaxIter()\n",
      " |  5\n",
      " |  >>> model_path = temp_path + \"/lr_model\"\n",
      " |  >>> model.save(model_path)\n",
      " |  >>> model2 = LinearRegressionModel.load(model_path)\n",
      " |  >>> model.coefficients[0] == model2.coefficients[0]\n",
      " |  True\n",
      " |  >>> model.intercept == model2.intercept\n",
      " |  True\n",
      " |  >>> model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
      " |  True\n",
      " |  >>> model.numFeatures\n",
      " |  1\n",
      " |  >>> model.write().format(\"pmml\").save(model_path + \"_2\")\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      _JavaRegressor\n",
      " |      Regressor\n",
      " |      pyspark.ml.wrapper.JavaPredictor\n",
      " |      pyspark.ml.base.Predictor\n",
      " |      pyspark.ml.wrapper.JavaEstimator\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Estimator\n",
      " |      _LinearRegressionParams\n",
      " |      pyspark.ml.base._PredictorParams\n",
      " |      pyspark.ml.param.shared.HasLabelCol\n",
      " |      pyspark.ml.param.shared.HasFeaturesCol\n",
      " |      pyspark.ml.param.shared.HasPredictionCol\n",
      " |      pyspark.ml.param.shared.HasRegParam\n",
      " |      pyspark.ml.param.shared.HasElasticNetParam\n",
      " |      pyspark.ml.param.shared.HasMaxIter\n",
      " |      pyspark.ml.param.shared.HasTol\n",
      " |      pyspark.ml.param.shared.HasFitIntercept\n",
      " |      pyspark.ml.param.shared.HasStandardization\n",
      " |      pyspark.ml.param.shared.HasWeightCol\n",
      " |      pyspark.ml.param.shared.HasSolver\n",
      " |      pyspark.ml.param.shared.HasAggregationDepth\n",
      " |      pyspark.ml.param.shared.HasLoss\n",
      " |      pyspark.ml.param.shared.HasMaxBlockSizeInMB\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0)\n",
      " |      __init__(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                  maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                  standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                  loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |  \n",
      " |  setAggregationDepth(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`aggregationDepth`.\n",
      " |  \n",
      " |  setElasticNetParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`elasticNetParam`.\n",
      " |  \n",
      " |  setEpsilon(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`epsilon`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  setFitIntercept(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`fitIntercept`.\n",
      " |  \n",
      " |  setLoss(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`loss`.\n",
      " |  \n",
      " |  setMaxBlockSizeInMB(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxBlockSizeInMB`.\n",
      " |      \n",
      " |      .. versionadded:: 3.1.0\n",
      " |  \n",
      " |  setMaxIter(self, value: int) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`maxIter`.\n",
      " |  \n",
      " |  setParams(self, *, featuresCol: str = 'features', labelCol: str = 'label', predictionCol: str = 'prediction', maxIter: int = 100, regParam: float = 0.0, elasticNetParam: float = 0.0, tol: float = 1e-06, fitIntercept: bool = True, standardization: bool = True, solver: str = 'auto', weightCol: Optional[str] = None, aggregationDepth: int = 2, loss: str = 'squaredError', epsilon: float = 1.35, maxBlockSizeInMB: float = 0.0) -> 'LinearRegression'\n",
      " |      setParams(self, \\*, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\",                   maxIter=100, regParam=0.0, elasticNetParam=0.0, tol=1e-6, fitIntercept=True,                   standardization=True, solver=\"auto\", weightCol=None, aggregationDepth=2,                   loss=\"squaredError\", epsilon=1.35, maxBlockSizeInMB=0.0)\n",
      " |      Sets params for linear regression.\n",
      " |      \n",
      " |      .. versionadded:: 1.4.0\n",
      " |  \n",
      " |  setRegParam(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`regParam`.\n",
      " |  \n",
      " |  setSolver(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`solver`.\n",
      " |  \n",
      " |  setStandardization(self, value: bool) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`standardization`.\n",
      " |  \n",
      " |  setTol(self, value: float) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`tol`.\n",
      " |  \n",
      " |  setWeightCol(self, value: str) -> 'LinearRegression'\n",
      " |      Sets the value of :py:attr:`weightCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_input_kwargs': typing.Dict[str, typing.Any]}\n",
      " |  \n",
      " |  __orig_bases__ = (pyspark.ml.regression._JavaRegressor[ForwardRef('Lin...\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Predictor:\n",
      " |  \n",
      " |  setFeaturesCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`featuresCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setLabelCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`labelCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  setPredictionCol(self: ~P, value: str) -> ~P\n",
      " |      Sets the value of :py:attr:`predictionCol`.\n",
      " |      \n",
      " |      .. versionadded:: 3.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  clear(self, param: pyspark.ml.param.Param) -> None\n",
      " |      Clears a param from the param map if it has been explicitly set.\n",
      " |  \n",
      " |  copy(self: 'JP', extra: Optional[ForwardRef('ParamMap')] = None) -> 'JP'\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          Extra parameters to copy to the new instance\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`JavaParams`\n",
      " |          Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __del__(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Estimator:\n",
      " |  \n",
      " |  fit(self, dataset: pyspark.sql.dataframe.DataFrame, params: Union[ForwardRef('ParamMap'), List[ForwardRef('ParamMap')], Tuple[ForwardRef('ParamMap')], NoneType] = None) -> Union[~M, List[~M]]\n",
      " |      Fits a model to the input dataset with optional parameters.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      params : dict or list or tuple, optional\n",
      " |          an optional param map that overrides embedded params. If a list/tuple of\n",
      " |          param maps is given, this calls fit on each param map and returns a list of\n",
      " |          models.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`Transformer` or a list of :py:class:`Transformer`\n",
      " |          fitted model(s)\n",
      " |  \n",
      " |  fitMultiple(self, dataset: pyspark.sql.dataframe.DataFrame, paramMaps: Sequence[ForwardRef('ParamMap')]) -> Iterator[Tuple[int, ~M]]\n",
      " |      Fits a model to the input dataset for each param map in `paramMaps`.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : :py:class:`pyspark.sql.DataFrame`\n",
      " |          input dataset.\n",
      " |      paramMaps : :py:class:`collections.abc.Sequence`\n",
      " |          A Sequence of param maps.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :py:class:`_FitMultipleIterator`\n",
      " |          A thread safe iterable which contains one model for each param map. Each\n",
      " |          call to `next(modelIterator)` will return `(index, model)` where model was fit\n",
      " |          using `paramMaps[index]`. `index` values may not be sequential.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  getEpsilon(self) -> float\n",
      " |      Gets the value of epsilon or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _LinearRegressionParams:\n",
      " |  \n",
      " |  epsilon = Param(parent='undefined', name='epsilon', doc='T...s. Must b...\n",
      " |  \n",
      " |  loss = Param(parent='undefined', name='loss', doc='The ...imized. Supp...\n",
      " |  \n",
      " |  solver = Param(parent='undefined', name='solver', doc='Th...ation. Sup...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  getLabelCol(self) -> str\n",
      " |      Gets the value of labelCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasLabelCol:\n",
      " |  \n",
      " |  labelCol = Param(parent='undefined', name='labelCol', doc='label colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  getFeaturesCol(self) -> str\n",
      " |      Gets the value of featuresCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFeaturesCol:\n",
      " |  \n",
      " |  featuresCol = Param(parent='undefined', name='featuresCol', doc='featu...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  getPredictionCol(self) -> str\n",
      " |      Gets the value of predictionCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasPredictionCol:\n",
      " |  \n",
      " |  predictionCol = Param(parent='undefined', name='predictionCol', doc='p...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  getRegParam(self) -> float\n",
      " |      Gets the value of regParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasRegParam:\n",
      " |  \n",
      " |  regParam = Param(parent='undefined', name='regParam', doc='regularizat...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  getElasticNetParam(self) -> float\n",
      " |      Gets the value of elasticNetParam or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasElasticNetParam:\n",
      " |  \n",
      " |  elasticNetParam = Param(parent='undefined', name='elasticNetParam'...L...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  getMaxIter(self) -> int\n",
      " |      Gets the value of maxIter or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxIter:\n",
      " |  \n",
      " |  maxIter = Param(parent='undefined', name='maxIter', doc='max number of...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  getTol(self) -> float\n",
      " |      Gets the value of tol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasTol:\n",
      " |  \n",
      " |  tol = Param(parent='undefined', name='tol', doc='the c...ence toleranc...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  getFitIntercept(self) -> bool\n",
      " |      Gets the value of fitIntercept or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasFitIntercept:\n",
      " |  \n",
      " |  fitIntercept = Param(parent='undefined', name='fitIntercept', doc='whe...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  getStandardization(self) -> bool\n",
      " |      Gets the value of standardization or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasStandardization:\n",
      " |  \n",
      " |  standardization = Param(parent='undefined', name='standardization'...t...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  getWeightCol(self) -> str\n",
      " |      Gets the value of weightCol or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasWeightCol:\n",
      " |  \n",
      " |  weightCol = Param(parent='undefined', name='weightCol', doc=...or empt...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasSolver:\n",
      " |  \n",
      " |  getSolver(self) -> str\n",
      " |      Gets the value of solver or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  getAggregationDepth(self) -> int\n",
      " |      Gets the value of aggregationDepth or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasAggregationDepth:\n",
      " |  \n",
      " |  aggregationDepth = Param(parent='undefined', name='aggregationDepth', ...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasLoss:\n",
      " |  \n",
      " |  getLoss(self) -> str\n",
      " |      Gets the value of loss or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  getMaxBlockSizeInMB(self) -> float\n",
      " |      Gets the value of maxBlockSizeInMB or its default value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasMaxBlockSizeInMB:\n",
      " |  \n",
      " |  maxBlockSizeInMB = Param(parent='undefined', name='maxBlockSizeInMB......\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param: Union[str, pyspark.ml.param.Param]) -> str\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self) -> str\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra: Optional[ForwardRef('ParamMap')] = None) -> 'ParamMap'\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      extra : dict, optional\n",
      " |          extra param values\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |          merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param: Union[str, pyspark.ml.param.Param[~T]]) -> Union[Any, ~T]\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName: str) -> pyspark.ml.param.Param\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName: str) -> bool\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param: Union[str, pyspark.ml.param.Param[Any]]) -> bool\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param: pyspark.ml.param.Param, value: Any) -> None\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self) -> pyspark.ml.util.JavaMLWriter\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path: str) -> None\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() -> pyspark.ml.util.JavaMLReader[~RL] from abc.ABCMeta\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path: str) -> ~RL from abc.ABCMeta\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from abc.ABCMeta\n",
      " |      Parameterizes a generic class.\n",
      " |      \n",
      " |      At least, parameterizing a generic class is the *main* thing this method\n",
      " |      does. For example, for some generic class `Foo`, this is called when we\n",
      " |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |      \n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from abc.ABCMeta\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "help(LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+--------------------+\n",
      "|carat|    cut|color|clarity|depth|table|price|   x|   y|   z|            features|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+--------------------+\n",
      "| 0.23|  Ideal|    E|    SI2| 61.5| 55.0|  326|3.95|3.98|2.43|[0.23,61.5,3.95,3...|\n",
      "| 0.21|Premium|    E|    SI1| 59.8| 61.0|  326|3.89|3.84|2.31|[0.21,59.8,3.89,3...|\n",
      "| 0.23|   Good|    E|    VS1| 56.9| 65.0|  327|4.05|4.07|2.31|[0.23,56.9,4.05,4...|\n",
      "+-----+-------+-----+-------+-----+-----+-----+----+----+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vectorizar las features de la entrada\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['carat', 'depth', 'x', 'y', 'z', 'table'],\n",
    "    outputCol='features' # le llamamos features para que coincida con lo que piden los algoritmos\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "df_assembled.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.23,61.5,3.95,3...|  326|\n",
      "|[0.21,59.8,3.89,3...|  326|\n",
      "|[0.23,56.9,4.05,4...|  327|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# seleccionar solo las columnas que usaremos para el modelado:\n",
    "# Entrada \"X\" sería \"features\" para pyspark\n",
    "# Salida \"y\" sería \"label\" para pyspark\n",
    "df_features_label = df_assembled.withColumnRenamed('price', 'label').select('features', 'label')\n",
    "df_features_label.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionamiento de datos\n",
    "df_train, df_test = df_features_label.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[0.22,59.3,3.91,3...|  404|2.9361297378709423|\n",
      "|[0.23,56.9,4.05,4...|  327|120.70671988210961|\n",
      "|[0.23,59.4,4.0,4....|  338| 86.91024750544966|\n",
      "|[0.23,60.5,3.96,3...|  357|-88.79676058197947|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "model = lr.fit(df_train)\n",
    "df_pred = model.transform(df_test)\n",
    "df_pred.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.859649017378561\n",
      "mae 879.5043945810185\n",
      "mse 2165793.592478833\n",
      "rmse 1471.6635459502397\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(metricName='mae')\n",
    "evaluator_mse = RegressionEvaluator(metricName='mse')\n",
    "evaluator_rmse = RegressionEvaluator(metricName='rmse')\n",
    "\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor, RandomForestRegressor, GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.8762063316673359\n",
      "mae 792.0061850657814\n",
      "mse 1910293.2423885928\n",
      "rmse 1382.133583409575\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor()\n",
    "model = tree.fit(df_train)\n",
    "df_pred = model.transform(df_test)\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.8780712896102745\n",
      "mae 789.0473691260198\n",
      "mse 1881514.5770196922\n",
      "rmse 1371.68311829653\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(numTrees=200)\n",
    "model = rf.fit(df_train)\n",
    "df_pred = model.transform(df_test)\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.8819099139029632\n",
      "mae 773.688666981911\n",
      "mse 1822279.7377491826\n",
      "rmse 1349.9184189235966\n"
     ]
    }
   ],
   "source": [
    "gbt = GBTRegressor()\n",
    "model = gbt.fit(df_train)\n",
    "df_pred = model.transform(df_test)\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
