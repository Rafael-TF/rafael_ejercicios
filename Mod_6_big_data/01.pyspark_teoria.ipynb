{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "\n",
    "**PySpark** es la interfaz de programación de **Python** para el framework de procesamiento distribuido **Apache Spark**.\n",
    "\n",
    "**Spark** es un motor de procesamiento de datos distribuido y de alto rendimiento que se utiliza para procesar grandes volúmenes de datos de manera escalable y eficiente en clústeres de computadoras.\n",
    "\n",
    "**PySpark** se utiliza comúnmente para tareas de procesamiento de datos, aprendizaje automático, análisis de datos en tiempo real, y para la construcción de aplicaciones de procesamiento de grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 17:14:45 WARN Utils: Your hostname, MacBook-Air-de-Rafael.local resolves to a loopback address: 127.0.0.1; using 192.168.100.10 instead (on interface en0)\n",
      "25/02/17 17:14:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/17 17:14:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x117c1a240>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@11/libexec/openjdk.jdk/Contents/Home\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark_teoria\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.10:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_teoria</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11bd89490>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark_teoria\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numero de nucleos\n",
    "cores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar un df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic.txt.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Leer un archivo con PySpark\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m titanic \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m                          inferSchema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(path)))\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic.txt."
     ]
    }
   ],
   "source": [
    "# Leer un archivo con PySpark\n",
    "titanic = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic.txt\",\n",
    "                         inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.show(5, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.schema[\"Ticket\"].dataType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.select(\"age\", \"fare\").summary(\"count\", \"min\", \"max\", \"mean\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Espeficicar dtypes de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark reconoce todos como strings\n",
    "\n",
    "people = spark.read.json(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/people.json\")\n",
    "\n",
    "print(people.printSchema())\n",
    "\n",
    "people.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos el dtype de \"timestamp\" a DateType()\n",
    "\n",
    "data_schema = list((StructField(\"name\"      , StringType(), True),\n",
    "                    StructField(\"email\"     , StringType(), True),\n",
    "                    StructField(\"city\"      , StringType(), True),\n",
    "                    StructField(\"mac\"       , StringType(), True),\n",
    "                    StructField(\"timestamp\" ,   DateType(), True),\n",
    "                    StructField(\"creditcard\", StringType(), True)))\n",
    "\n",
    "final_struc = StructType(fields = data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo otra vez pero especificando el schema\n",
    "\n",
    "people = spark.read.json(path   = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/people.json\",\n",
    "                         schema = final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar y Filtrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa = spark.read.csv(path        = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/fifa19.csv\",\n",
    "                      inferSchema = True, header = True)\n",
    "\n",
    "fifa.limit(4).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fifa.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para seleccionar columnas usamos .select y pasamos una lista con las columnas (los corchetes son opcionales)\n",
    "\n",
    "fifa.select([\"Nationality\", \"Name\", \"Age\", \"Photo\"]).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderBy, por defecto ascending = True\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"])\\\n",
    "    .orderBy(fifa[\"Age\"]).show(5)\n",
    "\n",
    "#fifa.select([\"Name\", \"Age\"])\\\n",
    "#    .orderBy(fifa[\"Age\"].asc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .desc()\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"])\\\n",
    "    .orderBy(fifa[\"Age\"].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para filtrar por palabras podemos usar .where en conjunto con .like\n",
    "\n",
    "fifa.select([\"Name\", \"Club\"])\\\n",
    "    .where(fifa.Club.like(\"%Barcelona%\")).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos utilizar .substr() para hacer \"slicing\" a una cadena de caracteres\n",
    "\n",
    "fifa.select(\"Photo\", fifa.Photo.substr(-4, 4)).show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .isin similar a Pandas\n",
    "\n",
    "fifa[fifa.Club.isin(\"FC Barcelona\", \"Juventus\")].limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .where(), .startswith() y .endswith()\n",
    "# Nota: los .where van uno detrás de otro.\n",
    "\n",
    "# fifa.select(\"Name\", \"Club\").where(fifa.Name.startswith(\"L\")).where(fifa.Name.endswith(\"i\")).show(5)\n",
    "\n",
    "fifa.select(\"Name\", \"Club\")                \\\n",
    "    .where(fifa.Name.startswith(\"L\"))      \\\n",
    "    .where(fifa.Name.endswith(\"i\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape[0]\n",
    "\n",
    "fifa.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .limit() para seleccionar el número de filas\n",
    "\n",
    "df3 = fifa.limit(100)\n",
    "df3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con las primeras 5 columnas\n",
    "\n",
    "col_list = fifa.columns[:5]\n",
    "df3 = fifa.select(col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuevo df\n",
    "df3.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .filter(condicion)\n",
    "\n",
    "fifa.filter(\"Overall > 50\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos usar .filter en conjunto con .select\n",
    "\n",
    "fifa.filter(\"Overall > 50\").select([\"Name\", \"Age\"]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El orden no afecta el output .select .filter\n",
    "\n",
    "fifa.select([\"Name\", \"Age\"]).filter(\"Overall > 50\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varias condiciones AND & OR\n",
    "\n",
    "fifa.select([\"Name\", \"Age\", \"Club\"]).filter(\"Overall > 50 AND Age < 30 AND Club = 'FC Barcelona'\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa.select([\"Name\", \"Age\", \"Club\"]).filter(\"Club = 'Juventus' OR Club = 'FC Barcelona'\").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .collect() \"transforma\" el output a list\n",
    "\n",
    "result = fifa.filter(\"Overall > 50\")                           \\\n",
    "             .select([\"Nationality\", \"Name\", \"Age\", \"Overall\"])\\\n",
    "             .orderBy(fifa[\"Overall\"].desc()).collect()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "print(\"Mejor jugador Overall>50\", result[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifa\n",
    "print(\"Mejor jugador Overall>50\", fifa[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "print(\"Peor jugador Overall<50\", result[-1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulacion de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# concat_ws()\n",
    "\n",
    "concat = fifa.select(fifa.Name,\n",
    "                     fifa.Nationality,\n",
    "                     concat_ws(\" \", fifa.Name, fifa.Nationality).alias(\"Nombre/Nacionalidad\"))\n",
    "\n",
    "concat.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat.rdd.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevo df\n",
    "\n",
    "videos = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/youtubevideos.csv\",\n",
    "                        header = True, inferSchema = True)\n",
    "\n",
    "videos.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos reasignar las columnas usando .withColumn en conjunto con .cast, to_date o to_timestamp\n",
    "\n",
    "df = videos.withColumn(\"views\"        , videos[\"views\"].cast(IntegerType()))                        \\\n",
    "           .withColumn(\"likes\"        , videos[\"likes\"].cast(IntegerType()))                        \\\n",
    "           .withColumn(\"dislikes\"     , videos[\"dislikes\"].cast(IntegerType()))                     \\\n",
    "           .withColumn(\"category_id\"  , videos[\"category_id\"].cast(IntegerType()))                  \\\n",
    "           .withColumn(\"trending_date\", to_date(videos.trending_date, \"yy.dd.mm\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumn() también nos permite crear columnas a partir de otras\n",
    "\n",
    "df = df.withColumn(\"publish_time_2\", regexp_replace(df.publish_time, \"T\", \" \"))\n",
    "df = df.withColumn(\"publish_time_2\", regexp_replace(df.publish_time_2, \"Z\", \"\"))\n",
    "\n",
    "df.select(\"publish_time\", \"publish_time_2\").show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower()\n",
    "df.select(\"title\", lower(df.title)).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when(), puede crear columnas a partir de otras si se cumple cierta condición\n",
    "\n",
    "df.select(\"likes\",\n",
    "          \"dislikes\",\n",
    "          (when(df.likes > df.dislikes, \"Good\").when(df.likes < df.dislikes, \"Bad\").when(df.likes == df.dislikes, \"Equal\")\\\n",
    "          .otherwise(\"Undetermined\")).alias(\"Favorability\")).show(5)\n",
    "\n",
    "# otherwise() se usa cuando no se resuelve la condicion, y esto puede suceder, por ejemplo, cuando hay NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expr\n",
    "\n",
    "# con expr podemos escribir en sintaxis SQL como queremos la nueva columna\n",
    "\n",
    "df.select(\"likes\",\n",
    "          \"dislikes\",\n",
    "          expr(\"CASE WHEN likes > dislikes THEN 'Good' \\\n",
    "                     WHEN dislikes > likes THEN 'Bad'  \\\n",
    "                     ELSE 'Undetermined' END           \\\n",
    "                AS Favorability\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year() y month()\n",
    "# Esto funciona porque la columna esta en formato DateType()\n",
    "\n",
    "df.select(\"trending_date\",\n",
    "          year(\"trending_date\").alias(\"year\"),\n",
    "          month(\"trending_date\").alias(\"month\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datediff()\n",
    "# Esto funciona porque las columnas estan en formato DateType()\n",
    "\n",
    "df.select(\"trending_date\",\n",
    "          \"publish_time_2\",\n",
    "          datediff(df.publish_time_2, df.trending_date)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split()\n",
    "array = df.select(\"title\",\n",
    "                  split(df.title, \" \").alias(\"split\"))\n",
    "\n",
    "array.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_contains parecido a \"in\" en python\n",
    "\n",
    "array.select(\"split\",\n",
    "             array_contains(array.split, \"(HBO)\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_distinct parecido a .unique() en Pandas\n",
    "\n",
    "array.select(\"title\", array_distinct(array.split)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array_remove eliminar un elemento de un array \n",
    "\n",
    "array.select(\"title\", array_remove(array.split, \"Presidency:\")).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos usar funciones para crear nuevas columnas\n",
    "\n",
    "from pyspark.sql.functions import udf          # user define functions\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El retorno de lambda \n",
    "\n",
    "def square(x):\n",
    "    return int(x**2)\n",
    "\n",
    "square_udf = udf(f          = lambda x : square(x),\n",
    "                 returnType = IntegerType())\n",
    "\n",
    "df.select(\"dislikes\",\n",
    "          square_udf(\"dislikes\").alias(\"dislikes**2\")).where(col(\"dislikes\").isNotNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Si ejecutamos sin usar .isNotNull() nos dará error porque hay NaN's\n",
    "# df.select(\"dislikes\", square_udf(\"dislikes\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# igual que la funcion .groupBy() y .agg() de pandas\n",
    "\n",
    "fifa.groupBy(\"Club\", \"Nationality\").agg({\"ID\" : \"count\"}).show(1_000, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_fifa = pd.read_csv(filepath_or_buffer = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/fifa19.csv\")\n",
    "\n",
    "df_fifa.groupby([\"Club\", \"Nationality\"]).agg({\"ID\" : \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta notación podemos agregar .alias a las columnas\n",
    "\n",
    "fifa.groupBy(\"Club\").agg(min(fifa.Age).alias(\"Min Age\"),\n",
    "                         max(fifa.Age).alias(\"Max Age\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con .summary() podemos obtener un resultado similar\n",
    "\n",
    "videos.select(\"views\", \"likes\", \"dislikes\")                                      \\\n",
    "      .summary(\"count\", \"min\", \"25%\", \"75%\", \"max\", \"stddev\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic1 = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic 1.csv\",\n",
    "                          inferSchema = True, header = True)\n",
    "\n",
    "titanic2 = spark.read.csv(path = \"/kaggle/input/d/danielwtummler/pyspark-teoria-practica/titanic 2.csv\",\n",
    "                          inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic1.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic2.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .union funciona como pd.concat, solo funciona para axis = 0\n",
    "# Los dfs deben tener la misma cantidad de columnas para funcionar\n",
    "# Agrega las filas\n",
    "\n",
    "titanic = titanic1.union(titanic1)\n",
    "\n",
    "print(titanic1.count())\n",
    "print(titanic.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Joins\n",
    "titanic = titanic1.join(other = titanic2, on = [\"PassengerId\"], how = \"inner\")\n",
    "\n",
    "titanic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos con isNull()\n",
    "\n",
    "titanic.select([\"Name\", \"PassengerId\", \"Age\"]).filter(titanic.Age.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta funcion podemos contar cuantas filas tienen NaN's\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "def null_value_calc(df):\n",
    "    null_columns_counts = list()\n",
    "    numRows = df.count()\n",
    "    \n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        \n",
    "        if (nullRows > 0):\n",
    "            temp = k, nullRows, (nullRows / numRows)*100\n",
    "            null_columns_counts.append(temp)\n",
    "            \n",
    "    return null_columns_counts\n",
    "\n",
    "null_columns_calc_list = null_value_calc(titanic)\n",
    "\n",
    "null_columns_calc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(data = null_columns_calc_list,\n",
    "                      schema = [\"Name\", \"Count\", \"Percent\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.na.drop() = df.dropna()\n",
    "\n",
    "titanic.na.drop().limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() sin parametros\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop().count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con threshold = 8\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(thresh = 8).count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con threshold = 6\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(thresh = 6).count()\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() podemos elegir por cual columna eliminar las filas\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(subset = [\"Age\"]).count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .na.drop() con how = \"all\" (toda la fila debe tener NaN's)\n",
    "\n",
    "og_len = titanic.count()\n",
    "drop_len = titanic.na.drop(how = \"all\").count()\n",
    "\n",
    "print(\"Filas eliminadas\", og_len - drop_len)\n",
    "print(\"Porcentaje de filas eliminadas\", (og_len - drop_len)/og_len*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na.fill(value), \"value\" debe coincidir con el dtype de la columna\n",
    "# Si esto no se cumple, na.fill() no hará nada\n",
    "\n",
    "titanic.na.fill(value = 9999).limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fila 6\n",
    "titanic.na.fill(value = \"NO AGE\").limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos hacer fill a una columna especifica\n",
    "\n",
    "titanic.na.fill(value = 9999, subset = [\"Age\"]).limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En una linea\n",
    "\n",
    "titanic.filter(titanic.Age.isNull()).na.fill(value = 9999, subset = [\"Age\"]).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambia los NaN's por el promedio de la columna\n",
    "\n",
    "def fill_with_mean(df, include = set()):\n",
    "    stats = df.agg(*(avg(c).alias(c) for c in df.columns if c in include))\n",
    "    \n",
    "    return df.na.fill(value = stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = fill_with_mean(titanic, [\"Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fila 6\n",
    "updated_df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
