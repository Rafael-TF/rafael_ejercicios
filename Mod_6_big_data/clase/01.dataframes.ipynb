{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "Apache Spark es un motor de análisis de datos distribuido y de propósito general que facilita el procesamiento rápido y escalable de grandes volúmenes de datos. Fue diseñado para superar las limitaciones de velocidad y flexibilidad que existían en tecnologías anteriores (como Hadoop MapReduce)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## PySpark vs Scikit Learn\n",
    "\n",
    "* Scikit-Learn usa NumPy y está optimizado para computación en RAM, por lo que maneja eficientemente datasets de hasta 1M de filas en máquinas modernas.\n",
    "\n",
    "* PySpark escala mejor porque usa procesamiento distribuido en varios nodos, pero en un solo nodo local, puede ser más lento debido a la sobrecarga de ejecución distribuida.\n",
    "\n",
    "Usa PySpark si...\n",
    "\n",
    "* Tu dataset tiene millones o miles de millones de filas.\n",
    "* Trabajas en un clúster de varios nodos y necesitas paralelización real.\n",
    "* Procesas datos directamente desde HDFS, S3, Cassandra, Delta Lake, etc.\n",
    "* Realizas operaciones ETL pesadas o pipeline de machine learning a gran escala.\n",
    "* ETL (Extracción Transformación y Carga)\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "* Tiene una API de alto nivel DataFrames muy similar a Pandas y se puede convertir a DataFrames de Pandas. No es 100 % igual pero es parecida.\n",
    "* Tiene una API de alto nivel que es SparkSQL que permite trabajar directamente con código SQL y hace lo mismo que haríamos con DataFrames.\n",
    "* Tiene MLlib para Machine Learning muy parecido a Scikit Learn.\n",
    "\n",
    "Desventajas:\n",
    "* Mayor overhead, cuando ejecutamos algo primero se tiene que traducir de python al lenguaje que usa internamente Apache Spark para poder procesarlo, que es Java/Scala. De ahí se pasa a un DAG que genera las transformaciones y acciones. Optimización DAG, internamente mejora el plan a ejecutar. Ejecución distribuida: Spark divide el trabajo en tareas y las ejecuta en el clúster. Recolección del resultado que se envía al Driver o se guardan en bases de datos.\n",
    "* Complejidad de montar un cluster y mantenerlo.\n",
    "* Cantidades masivas de datos, si no es el caso no sería necesario.\n",
    "* Las ETL se pueden volver muy complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En pyspark las operaciones se dividen en dos tipos:\n",
    "\n",
    "* Transformaciones (Lazy Evaluation) (operaciones intermedias): genera un nuevo DataFrame o un RDD pero no se ejecutan ni devuelven inmediatamente.\n",
    "* Acción (operaciones terminales): ejecutan el plan de cómputo y devuelven un resultado al Driver o almacenan datos. Ejemplo: \n",
    "    * collect(): trae todos los datos en forma de lista. Cuidado porque al cargar todos los datos si son muchos podemos agotar recursos. Puede consumir mucha memoria.\n",
    "    * take(n)\n",
    "    * first()\n",
    "    * head(n)\n",
    "    * count()\n",
    "    * show(n)\n",
    "    * write.format().save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6bd88ec6b4fa:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_teoria</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb23b5d0510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"pyspark_teoria\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(sns.load_dataset('tips'))\n",
    "df.show(5) # equivalente a head de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34</td>\n",
       "      <td>1.66</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.01</td>\n",
       "      <td>3.50</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.68</td>\n",
       "      <td>3.31</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.59</td>\n",
       "      <td>3.61</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>25.29</td>\n",
       "      <td>4.71</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.77</td>\n",
       "      <td>2.00</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>26.88</td>\n",
       "      <td>3.12</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.04</td>\n",
       "      <td>1.96</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.78</td>\n",
       "      <td>3.23</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>Sun</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_bill   tip     sex smoker  day    time  size\n",
       "0       16.99  1.01  Female     No  Sun  Dinner     2\n",
       "1       10.34  1.66    Male     No  Sun  Dinner     3\n",
       "2       21.01  3.50    Male     No  Sun  Dinner     3\n",
       "3       23.68  3.31    Male     No  Sun  Dinner     2\n",
       "4       24.59  3.61  Female     No  Sun  Dinner     4\n",
       "5       25.29  4.71    Male     No  Sun  Dinner     4\n",
       "6        8.77  2.00    Male     No  Sun  Dinner     2\n",
       "7       26.88  3.12    Male     No  Sun  Dinner     4\n",
       "8       15.04  1.96    Male     No  Sun  Dinner     2\n",
       "9       14.78  3.23    Male     No  Sun  Dinner     2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convertir a dataframe de pandas\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+\n",
      "|total_bill| tip|   sex|\n",
      "+----------+----+------+\n",
      "|     16.99|1.01|Female|\n",
      "|     10.34|1.66|  Male|\n",
      "|     21.01| 3.5|  Male|\n",
      "|     23.68|3.31|  Male|\n",
      "|     24.59|3.61|Female|\n",
      "+----------+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df[['total_bill', 'tip', 'sex']].show(5)\n",
    "df.select('total_bill', 'tip', 'sex').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>total_bill</th>\n",
       "      <th>tip</th>\n",
       "      <th>sex</th>\n",
       "      <th>smoker</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>19.78594262295082</td>\n",
       "      <td>2.9982786885245902</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2.569672131147541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>8.902411954856856</td>\n",
       "      <td>1.383638189001182</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.9510998047322345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>No</td>\n",
       "      <td>Fri</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>50.81</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Thur</td>\n",
       "      <td>Lunch</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary         total_bill                 tip     sex smoker   day    time  \\\n",
       "0   count                244                 244     244    244   244     244   \n",
       "1    mean  19.78594262295082  2.9982786885245902    None   None  None    None   \n",
       "2  stddev  8.902411954856856   1.383638189001182    None   None  None    None   \n",
       "3     min               3.07                 1.0  Female     No   Fri  Dinner   \n",
       "4     max              50.81                10.0    Male    Yes  Thur   Lunch   \n",
       "\n",
       "                 size  \n",
       "0                 244  \n",
       "1   2.569672131147541  \n",
       "2  0.9510998047322345  \n",
       "3                   1  \n",
       "4                   6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bill', 'double'),\n",
       " ('tip', 'double'),\n",
       " ('sex', 'string'),\n",
       " ('smoker', 'string'),\n",
       " ('day', 'string'),\n",
       " ('time', 'string'),\n",
       " ('size', 'bigint')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructField('total_bill', DoubleType(), True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n",
      "\n",
      "withColumn(colName: str, col: pyspark.sql.column.Column) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      "    existing column that has the same name.\n",
      "    \n",
      "    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n",
      "    a column from some other :class:`DataFrame` will raise an error.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    colName : str\n",
      "        string, name of the new column.\n",
      "    col : :class:`Column`\n",
      "        a :class:`Column` expression for the new column.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        DataFrame with new or replaced column.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This method introduces a projection internally. Therefore, calling it multiple\n",
      "    times, for instance, via loops in order to add multiple columns can generate big\n",
      "    plans which can cause performance issues and even `StackOverflowException`.\n",
      "    To avoid this, use :func:`select` with multiple columns at once.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    >>> df.withColumn('age2', df.age + 2).show()\n",
      "    +---+-----+----+\n",
      "    |age| name|age2|\n",
      "    +---+-----+----+\n",
      "    |  2|Alice|   4|\n",
      "    |  5|  Bob|   7|\n",
      "    +---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: float (nullable = true)\n",
      " |-- tip: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conversión de tipos de datos, en pandas solemos usar astype()\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "# df_cast = df.withColumn('total_bill', col('total_bill').cast('float')) \\\n",
    "#             .withColumn('tip', col('tip').cast('integer'))\n",
    "  \n",
    "df_cast = df.withColumn('total_bill', col('total_bill').cast(FloatType())) \\\n",
    "        .withColumn('tip', col('tip').cast(IntegerType()))\n",
    "            \n",
    "df_cast.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+\n",
      "|summary|       total_bill|               tip|             size|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "|  count|              244|               244|              244|\n",
      "|    min|             3.07|               1.0|                1|\n",
      "|    max|            50.81|              10.0|                6|\n",
      "|   mean|19.78594262295082|2.9982786885245902|2.569672131147541|\n",
      "+-------+-----------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agregaciones\n",
    "df.select('total_bill', 'tip', 'size').summary('count', 'min', 'max', 'mean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "|summary|       total_bill|               tip|   sex|smoker| day|  time|              size|\n",
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "|  count|              244|               244|   244|   244| 244|   244|               244|\n",
      "|   mean|19.78594262295082|2.9982786885245902|  NULL|  NULL|NULL|  NULL| 2.569672131147541|\n",
      "| stddev|8.902411954856856| 1.383638189001182|  NULL|  NULL|NULL|  NULL|0.9510998047322345|\n",
      "|    min|             3.07|               1.0|Female|    No| Fri|Dinner|                 1|\n",
      "|    25%|            13.28|               2.0|  NULL|  NULL|NULL|  NULL|                 2|\n",
      "|    50%|            17.78|              2.88|  NULL|  NULL|NULL|  NULL|                 2|\n",
      "|    75%|            24.08|              3.55|  NULL|  NULL|NULL|  NULL|                 3|\n",
      "|    max|            50.81|              10.0|  Male|   Yes|Thur| Lunch|                 6|\n",
      "+-------+-----------------+------------------+------+------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# equivalente al describe de pandas\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method filter in module pyspark.sql.dataframe:\n",
      "\n",
      "filter(condition: 'ColumnOrName') -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n",
      "    Filters rows using the given condition.\n",
      "    \n",
      "    :func:`where` is an alias for :func:`filter`.\n",
      "    \n",
      "    .. versionadded:: 1.3.0\n",
      "    \n",
      "    .. versionchanged:: 3.4.0\n",
      "        Supports Spark Connect.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    condition : :class:`Column` or str\n",
      "        a :class:`Column` of :class:`types.BooleanType`\n",
      "        or a string of SQL expressions.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    :class:`DataFrame`\n",
      "        Filtered DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.createDataFrame([\n",
      "    ...     (2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n",
      "    \n",
      "    Filter by :class:`Column` instances.\n",
      "    \n",
      "    >>> df.filter(df.age > 3).show()\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |  5| Bob|\n",
      "    +---+----+\n",
      "    >>> df.where(df.age == 2).show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "    \n",
      "    Filter by SQL expression in a string.\n",
      "    \n",
      "    >>> df.filter(\"age > 3\").show()\n",
      "    +---+----+\n",
      "    |age|name|\n",
      "    +---+----+\n",
      "    |  5| Bob|\n",
      "    +---+----+\n",
      "    >>> df.where(\"age = 2\").show()\n",
      "    +---+-----+\n",
      "    |age| name|\n",
      "    +---+-----+\n",
      "    |  2|Alice|\n",
      "    +---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "|     20.29|2.75|Female|    No|Sat|Dinner|   2|\n",
      "|     39.42|7.58|  Male|    No|Sat|Dinner|   4|\n",
      "|      21.7| 4.3|  Male|    No|Sat|Dinner|   2|\n",
      "|     20.69|2.45|Female|    No|Sat|Dinner|   4|\n",
      "|     24.06| 3.6|  Male|    No|Sat|Dinner|   3|\n",
      "|     31.27| 5.0|  Male|    No|Sat|Dinner|   3|\n",
      "|      30.4| 5.6|  Male|    No|Sun|Dinner|   4|\n",
      "|     22.23| 5.0|  Male|    No|Sun|Dinner|   2|\n",
      "|      32.4| 6.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     28.55|2.05|  Male|    No|Sun|Dinner|   3|\n",
      "|     34.81| 5.2|Female|    No|Sun|Dinner|   4|\n",
      "|     25.56|4.34|  Male|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filtro por una columna\n",
    "# df.filter(df.total_bill > 20).show()\n",
    "df.filter(df['total_bill'] > 20).show()\n",
    "# df.filter(col('total_bill') > 20).show()\n",
    "\n",
    "# df.filter(df['total_bill'] > 20).collect()[0]['total_bill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+------+---+------+----+\n",
      "|total_bill| tip| sex|smoker|day|  time|size|\n",
      "+----------+----+----+------+---+------+----+\n",
      "|     21.01| 3.5|Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+----+------+---+------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['total_bill'] > 20) & (df['tip'] > 3)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size| total_bill_iva_10|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|1.6989999999999998|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|             1.034|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear una nueva columna con el 10 % de total_bill para el IVA\n",
    "df_new = df.withColumn('total_bill_iva_10', df['total_bill'] * 0.10)\n",
    "df_new.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|tip_category|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|       media|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|       media|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        alta|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        alta|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        alta|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        alta|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        alta|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|       media|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        alta|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|       media|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        alta|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        alta|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        alta|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en pandas solemos aplicar una transformación utilizando apply()\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "# crear columna categórica a partir de numérica\n",
    "\n",
    "# df.withColumn(\n",
    "#     'tip_category', \n",
    "#     when(df['tip'] <= 1, 'baja')\n",
    "#     .when((df['tip'] > 1) & (df['tip'] <= 3), 'media')\n",
    "#     .otherwise('alta')\n",
    "# ).show()\n",
    "\n",
    "df.withColumn(\n",
    "    'tip_category', \n",
    "    when(col('tip') <= 1, 'baja')\n",
    "    .when((col('tip') > 1) & (col('tip') <= 3), 'media')\n",
    "    .otherwise('alta')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|tip_category|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|       media|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|       media|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        alta|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        alta|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|        alta|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|        alta|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|        alta|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|       media|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|       media|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|        alta|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|        alta|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|       media|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|        alta|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|        alta|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|        alta|\n",
      "+----------+----+------+------+---+------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alternativa al ejemplo anterior usando User Defined Function (UDF)\n",
    "# Esto es mejor solo para casos avanzados en los que no nos sirve con las funciones que ya hay en functions\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def categorize_tip(tip):\n",
    "    if tip <= 1:\n",
    "        return 'baja'\n",
    "    elif tip > 1 and tip <= 3:\n",
    "        return 'media'\n",
    "    else:\n",
    "        return 'alta'\n",
    "    \n",
    "udf_categorize_tip = udf(categorize_tip, StringType())\n",
    "df.withColumn('tip_category', udf_categorize_tip('tip')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip| genre|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renombrar columnas\n",
    "df_renamed = df.withColumnRenamed('sex','genre')\n",
    "df_renamed.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+------+----+\n",
      "|total_bill| tip|day|  time|size|\n",
      "+----------+----+---+------+----+\n",
      "|     16.99|1.01|Sun|Dinner|   2|\n",
      "|     10.34|1.66|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|Sun|Dinner|   3|\n",
      "|     23.68|3.31|Sun|Dinner|   2|\n",
      "+----------+----+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dropped = df.drop('sex', 'smoker')\n",
    "df_dropped.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+------+---+------+----+\n",
      "|total_bill|tip|   sex|smoker|day|  time|size|\n",
      "+----------+---+------+------+---+------+----+\n",
      "|      3.07|1.0|Female|   Yes|Sat|Dinner|   1|\n",
      "|      5.75|1.0|Female|   Yes|Fri|Dinner|   2|\n",
      "|      7.25|1.0|Female|    No|Sat|Dinner|   1|\n",
      "+----------+---+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+----+----+------+---+------+----+\n",
      "|total_bill| tip| sex|smoker|day|  time|size|\n",
      "+----------+----+----+------+---+------+----+\n",
      "|     50.81|10.0|Male|   Yes|Sat|Dinner|   3|\n",
      "|     48.33| 9.0|Male|    No|Sat|Dinner|   4|\n",
      "|     48.27|6.73|Male|    No|Sat|Dinner|   4|\n",
      "+----------+----+----+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+----+----+------+---+------+----+\n",
      "|total_bill| tip| sex|smoker|day|  time|size|\n",
      "+----------+----+----+------+---+------+----+\n",
      "|     50.81|10.0|Male|   Yes|Sat|Dinner|   3|\n",
      "|     48.33| 9.0|Male|    No|Sat|Dinner|   4|\n",
      "|     48.27|6.73|Male|    No|Sat|Dinner|   4|\n",
      "+----------+----+----+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ordenar por una columna: en pandas es sort_values\n",
    "df.sort('total_bill').show(3) # asc\n",
    "df.sort(col('total_bill').desc()).show(3)\n",
    "df.orderBy(col('total_bill').desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|count|\n",
      "+------+-----+\n",
      "|Female|   87|\n",
      "|  Male|  157|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agrupar datos\n",
    "# equivalente a value_counts de pandas\n",
    "df.groupBy('sex').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------------------+------------------+\n",
      "|   sex|count_rows|    avg_total_bill|          sum_tips|\n",
      "+------+----------+------------------+------------------+\n",
      "|Female|        87|18.056896551724137|            246.51|\n",
      "|  Male|       157| 20.74407643312102|485.07000000000005|\n",
      "+------+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#similar a pandas podemos usar una función agg para pedir varias agregaciones\n",
    "from pyspark.sql.functions import avg, count, sum\n",
    "\n",
    "df.groupby('sex').agg(\n",
    "    count('*').alias('count_rows'),\n",
    "    avg('total_bill').alias('avg_total_bill'),\n",
    "    sum('tip').alias('sum_tips')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina filas donde hay al menos un valor nulo:\n",
    "df_no_nulls = df.dropna()\n",
    "\n",
    "# Eliminar filas donde hay nulos solo en algunas columnas especificadas:\n",
    "df_no_nulls = df.dropna(subset=['tip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rellenar nulos\n",
    "df_imputed = df.fillna({\n",
    "    'total_bill': 0,\n",
    "    'smoker': 'desconocido'\n",
    "})\n",
    "df_imputed.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar CSV desde pandas y luego a pyspark\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n",
    "df_pandas = pd.read_csv(url)\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar CSV directamente con pyspark (Más recomendable)\n",
    "import requests\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n",
    "# csv_path= '/tmp/tips.csv'\n",
    "csv_path= 'tips.csv'\n",
    "\n",
    "with open(csv_path, 'wb') as file: # w de write b de binary\n",
    "    file.write(requests.get(url).content)\n",
    "    \n",
    "df_spark = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- total_bill: float (nullable = true)\n",
      " |-- tip: float (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar CSV directamente con pyspark + schema (Más recomendable)\n",
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, StringType, IntegerType\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n",
    "# csv_path= '/tmp/tips.csv'\n",
    "csv_path= 'tips.csv'\n",
    "\n",
    "with open(csv_path, 'wb') as file: # w de write b de binary\n",
    "    file.write(requests.get(url).content)\n",
    "    \n",
    "schema = StructType([\n",
    "    # columnas del dataset y su tipo de dato\n",
    "    StructField('total_bill', FloatType(), True),\n",
    "    StructField('tip', FloatType(), True),\n",
    "    StructField('sex', StringType(), True),\n",
    "    StructField('smoker', StringType(), True),\n",
    "    StructField('day', StringType(), True),\n",
    "    StructField('time', StringType(), True),\n",
    "    StructField('size', IntegerType(), True)\n",
    "])\n",
    "    \n",
    "df_spark = spark.read.csv(csv_path, header=True, inferSchema=False, schema=schema)\n",
    "df_spark.show(5)\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar datos a un csv\n",
    "# por defecto se particiona en múltiples archivos para procesado distribuido y repartirlos en nodos\n",
    "# luego a la hora de leerlo spark detecta automáticamente que el archivo está particionado y lo lee bien\n",
    "df.write.csv('tips_clean.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducir a una sola partición (No recomendable)\n",
    "df.coalesce(1).write.csv('tips_clean2.csv', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".bash_logout\n",
      ".bashrc\n",
      ".profile\n",
      ".ipython\n",
      ".npm\n",
      ".cache\n",
      ".ipynb_checkpoints\n",
      "tips.csv\n",
      ".local\n",
      ".jupyter\n",
      "tips_clean.csv\n",
      ".conda\n",
      ".config\n",
      ".wget-hsts\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "# verificar que aparece el archivo guardado:\n",
    "import os\n",
    "\n",
    "files = os.listdir('.')\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------+------+---+------+----+\n",
      "|total_bill|tip|   sex|smoker|day|  time|size|\n",
      "+----------+---+------+------+---+------+----+\n",
      "|     16.27|2.5|Female|   Yes|Fri| Lunch|   2|\n",
      "|     10.09|2.0|Female|   Yes|Fri| Lunch|   2|\n",
      "|     20.45|3.0|  Male|    No|Sat|Dinner|   4|\n",
      "+----------+---+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tips_clean = spark.read.csv('tips_clean.csv', header=True, inferSchema=True)\n",
    "df_tips_clean.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # se puede conectar con otras fuentes de datos, como MySQL\n",
    "# spark = SparkSession.builder.appName('mysqlapp').config('spark.jars', '/opt/mysql-connector-java-8.0.41.jar').getOrCreate()\n",
    "\n",
    "# # java database connectivity\n",
    "# #.option('dbtable', 'customers')\n",
    "\n",
    "# df_mysql = spark.read.format('jdbc') \\\n",
    "#           .option('url', 'jdbc:mysql://localhost:3306/testing_db') \\\n",
    "#           .option('query', 'SELECT * FROM customers WHERE salary > 1000') \\\n",
    "#           .option('user', 'root') \\\n",
    "#           .option('password', 'admin') \\\n",
    "#           .load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
