{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión mpg\n",
    "\n",
    "Regresión con pyspark para predecir columna mpg del dataset mpg.\n",
    "\n",
    "Importante el uso de VectorAssembler.\n",
    "\n",
    "Por ejemplo, en un DataFrame de Spark, la librería no espera que tengas 4, 5 o 6 columnas separadas para las variables explicativas, sino una sola columna del tipo Vector que contenga esos 4, 5 o 6 valores.\n",
    "\n",
    "Spark provee VectorAssembler, que usa varias columnas numéricas (o categóricas transformadas) y las combina en un solo vector llamado \"features\".\n",
    "\n",
    "Internamente, VectorAssembler crea (por cada fila) un DenseVector o SparseVector con todas las columnas que le pasas. Así, en la etapa de entrenamiento, el modelo lee esa columna vectorial y la columna de label (la que quieres predecir).\n",
    "\n",
    "DataFrames:\n",
    "\n",
    "* pyspark.sql.DataFrame (el DataFrame \"real\" de Spark):\n",
    "    * Este es el tipo que entienden todos los métodos de ML de PySpark (.ml o .mllib).\n",
    "    * Permite paralelizar operaciones, transformaciones, etc. en un clúster Spark.\n",
    "    * Tiene sus propios métodos (.select, .withColumn, .show, etc.).\n",
    "    * Tenemos que crear column a features y label para trabajar con los modelos de MLlib habitualmente.\n",
    "\n",
    "* pyspark.pandas.DataFrame (o ps.DataFrame):\n",
    "\n",
    "    * Es un DataFrame con API parecida a la de pandas (intenta imitar la sintaxis de pandas).\n",
    "    * Por debajo, se apoya en Spark, pero no es directamente compatible con la librería ML.\n",
    "    * Existe para hacer más fácil la transición a Spark de un usuario que maneje pandas, pero no permite usar los algoritmos de Spark ML directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model_year|origin|                name|\n",
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0|  3504|        12.0|        70|   usa|chevrolet chevell...|\n",
      "|15.0|        8|       350.0|     165.0|  3693|        11.5|        70|   usa|   buick skylark 320|\n",
      "|18.0|        8|       318.0|     150.0|  3436|        11.0|        70|   usa|  plymouth satellite|\n",
      "|16.0|        8|       304.0|     150.0|  3433|        12.0|        70|   usa|       amc rebel sst|\n",
      "|17.0|        8|       302.0|     140.0|  3449|        10.5|        70|   usa|         ford torino|\n",
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('regresion_mpg').getOrCreate()\n",
    "df = spark.createDataFrame(sns.load_dataset('mpg').dropna()) # Le quitamos los nulos\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+--------------------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model_year|origin|                name|            features|\n",
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+--------------------+\n",
      "|18.0|        8|       307.0|     130.0|  3504|        12.0|        70|   usa|chevrolet chevell...|[8.0,307.0,130.0,...|\n",
      "|15.0|        8|       350.0|     165.0|  3693|        11.5|        70|   usa|   buick skylark 320|[8.0,350.0,165.0,...|\n",
      "|18.0|        8|       318.0|     150.0|  3436|        11.0|        70|   usa|  plymouth satellite|[8.0,318.0,150.0,...|\n",
      "+----+---------+------------+----------+------+------------+----------+------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# opción 1: hacer assembler antes de particionar datos\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year'],\n",
    "    outputCol='features' # le llamamos features para que coincida con lo que piden los algoritmos\n",
    ")\n",
    "df_assembled = assembler.transform(df)\n",
    "df_assembled.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[8.0,307.0,130.0,...| 18.0|\n",
      "|[8.0,350.0,165.0,...| 15.0|\n",
      "|[8.0,318.0,150.0,...| 18.0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features_label = df_assembled.withColumnRenamed('mpg', 'label').select('features', 'label')\n",
    "df_features_label.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df_features_label.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción 2: primero particionar y luego usar VectorAssembler\n",
    "\n",
    "numeric_cols = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']\n",
    "label_col = 'mpg'\n",
    "\n",
    "df_selected = df.select(numeric_cols + [label_col])\n",
    "# df_selected.show(1)\n",
    "\n",
    "df_train, df_test = df_selected.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_cols,\n",
    "    outputCol='features'\n",
    ")\n",
    "# Le hemos dejado como nombre 'mpg' en lugar de 'label' para ver\n",
    "# cómo usarlo en los algoritmos ML\n",
    "df_train = assembler.transform(df_train).select('features', label_col)\n",
    "df_test = assembler.transform(df_test).select('features', label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features| mpg|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[4.0,104.0,95.0,2...|25.0|22.759564401576107|\n",
      "|[4.0,121.0,113.0,...|26.0| 23.26698094028353|\n",
      "|[6.0,199.0,97.0,2...|18.0| 20.16496724690829|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "# como no lo hemos renombrado a label tenemos que poner su nombre completo\n",
    "lr = LinearRegression(labelCol=label_col)\n",
    "model = lr.fit(df_train)\n",
    "df_pred = model.transform(df_test)\n",
    "df_pred.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 0.8008919249079934\n",
      "mae 2.496475133849499\n",
      "mse 10.002107194842727\n",
      "rmse 3.1626108193773588\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(metricName='r2', labelCol=label_col)\n",
    "evaluator_mae = RegressionEvaluator(metricName='mae', labelCol=label_col)\n",
    "evaluator_mse = RegressionEvaluator(metricName='mse', labelCol=label_col)\n",
    "evaluator_rmse = RegressionEvaluator(metricName='rmse', labelCol=label_col)\n",
    "\n",
    "print('r2', evaluator_r2.evaluate(df_pred))\n",
    "print('mae', evaluator_mae.evaluate(df_pred))\n",
    "print('mse', evaluator_mse.evaluate(df_pred))\n",
    "print('rmse', evaluator_rmse.evaluate(df_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mpg',\n",
       " 'cylinders',\n",
       " 'displacement',\n",
       " 'horsepower',\n",
       " 'weight',\n",
       " 'acceleration',\n",
       " 'model_year']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detectar nombres de columnas numéricas automáticamente, select_dtypes de pandas:\n",
    "from pyspark.sql.types import NumericType \n",
    "\n",
    "numeric_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+----------+------+------------+----------+\n",
      "| mpg|cylinders|displacement|horsepower|weight|acceleration|model_year|\n",
      "+----+---------+------------+----------+------+------------+----------+\n",
      "|18.0|        8|       307.0|     130.0|  3504|        12.0|        70|\n",
      "+----+---------+------------+----------+------+------------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(numeric_cols).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|origin|                name|\n",
      "+------+--------------------+\n",
      "|   usa|chevrolet chevell...|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType \n",
    "\n",
    "categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "df_categorical = df.select(categorical_cols)\n",
    "df_categorical.show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
